[INFO] compiling model...
Model: "model"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 128, 128, 3)]     0         
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 128, 128, 64)      1792      
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 128, 128, 64)      36928     
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 64, 64, 64)        0         
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 64, 64, 128)       73856     
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 64, 64, 128)       147584    
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 32, 32, 128)       0         
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 32, 32, 256)       295168    
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 32, 32, 256)       590080    
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 32, 32, 256)       590080    
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 16, 16, 256)       0         
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 16, 16, 512)       1180160   
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 16, 16, 512)       2359808   
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 16, 16, 512)       2359808   
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 8, 8, 512)         0         
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 8, 8, 512)         2359808   
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 8, 8, 512)         2359808   
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 8, 8, 512)         2359808   
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         
_________________________________________________________________
average_pooling2d (AveragePo (None, 1, 1, 512)         0         
_________________________________________________________________
flatten (Flatten)            (None, 512)               0         
_________________________________________________________________
dense (Dense)                (None, 64)                32832     
_________________________________________________________________
dropout (Dropout)            (None, 64)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 2)                 130       
=================================================================
Total params: 14,747,650
Trainable params: 32,962
Non-trainable params: 14,714,688
_________________________________________________________________
[INFO] training head...
  1/126 [..............................] - ETA: 1:23 - loss: 0.5521 - accuracy: 0.8750  2/126 [..............................] - ETA: 30s - loss: 0.5453 - accuracy: 0.8750   3/126 [..............................] - ETA: 30s - loss: 0.5379 - accuracy: 0.8750  4/126 [..............................] - ETA: 30s - loss: 0.5532 - accuracy: 0.8594  5/126 [>.............................] - ETA: 30s - loss: 0.5570 - accuracy: 0.8525  6/126 [>.............................] - ETA: 30s - loss: 0.5571 - accuracy: 0.8458  7/126 [>.............................] - ETA: 29s - loss: 0.5540 - accuracy: 0.8423  8/126 [>.............................] - ETA: 29s - loss: 0.5484 - accuracy: 0.8406  9/126 [=>............................] - ETA: 29s - loss: 0.5496 - accuracy: 0.8351 10/126 [=>............................] - ETA: 29s - loss: 0.5481 - accuracy: 0.8316 11/126 [=>............................] - ETA: 28s - loss: 0.5470 - accuracy: 0.8283 12/126 [=>............................] - ETA: 28s - loss: 0.5463 - accuracy: 0.8253 13/126 [==>...........................] - ETA: 28s - loss: 0.5506 - accuracy: 0.8202 14/126 [==>...........................] - ETA: 28s - loss: 0.5544 - accuracy: 0.8158 15/126 [==>...........................] - ETA: 27s - loss: 0.5572 - accuracy: 0.8126 16/126 [==>...........................] - ETA: 27s - loss: 0.5589 - accuracy: 0.8101 17/126 [===>..........................] - ETA: 27s - loss: 0.5611 - accuracy: 0.8075 18/126 [===>..........................] - ETA: 27s - loss: 0.5632 - accuracy: 0.8046 19/126 [===>..........................] - ETA: 27s - loss: 0.5647 - accuracy: 0.8025 20/126 [===>..........................] - ETA: 26s - loss: 0.5656 - accuracy: 0.8008 21/126 [====>.........................] - ETA: 26s - loss: 0.5663 - accuracy: 0.7989 22/126 [====>.........................] - ETA: 26s - loss: 0.5671 - accuracy: 0.7970 23/126 [====>.........................] - ETA: 26s - loss: 0.5675 - accuracy: 0.7954 24/126 [====>.........................] - ETA: 25s - loss: 0.5674 - accuracy: 0.7942 25/126 [====>.........................] - ETA: 25s - loss: 0.5677 - accuracy: 0.7928 26/126 [=====>........................] - ETA: 25s - loss: 0.5677 - accuracy: 0.7919 27/126 [=====>........................] - ETA: 25s - loss: 0.5674 - accuracy: 0.7910 28/126 [=====>........................] - ETA: 25s - loss: 0.5673 - accuracy: 0.7902 29/126 [=====>........................] - ETA: 24s - loss: 0.5670 - accuracy: 0.7895 30/126 [======>.......................] - ETA: 24s - loss: 0.5664 - accuracy: 0.7892 31/126 [======>.......................] - ETA: 24s - loss: 0.5655 - accuracy: 0.7891 32/126 [======>.......................] - ETA: 24s - loss: 0.5643 - accuracy: 0.7892 33/126 [======>.......................] - ETA: 24s - loss: 0.5631 - accuracy: 0.7894 34/126 [=======>......................] - ETA: 23s - loss: 0.5619 - accuracy: 0.7897 35/126 [=======>......................] - ETA: 24s - loss: 0.5606 - accuracy: 0.7900 36/126 [=======>......................] - ETA: 24s - loss: 0.5592 - accuracy: 0.7904 37/126 [=======>......................] - ETA: 24s - loss: 0.5578 - accuracy: 0.7908 38/126 [========>.....................] - ETA: 24s - loss: 0.5564 - accuracy: 0.7913 39/126 [========>.....................] - ETA: 24s - loss: 0.5548 - accuracy: 0.7919 40/126 [========>.....................] - ETA: 24s - loss: 0.5532 - accuracy: 0.7925 41/126 [========>.....................] - ETA: 24s - loss: 0.5517 - accuracy: 0.7931 42/126 [=========>....................] - ETA: 24s - loss: 0.5501 - accuracy: 0.7937 43/126 [=========>....................] - ETA: 23s - loss: 0.5488 - accuracy: 0.7942 44/126 [=========>....................] - ETA: 23s - loss: 0.5475 - accuracy: 0.7946 45/126 [=========>....................] - ETA: 23s - loss: 0.5462 - accuracy: 0.7950 46/126 [=========>....................] - ETA: 23s - loss: 0.5452 - accuracy: 0.7952 47/126 [==========>...................] - ETA: 23s - loss: 0.5442 - accuracy: 0.7955 48/126 [==========>...................] - ETA: 23s - loss: 0.5432 - accuracy: 0.7957 49/126 [==========>...................] - ETA: 23s - loss: 0.5424 - accuracy: 0.7959 50/126 [==========>...................] - ETA: 23s - loss: 0.5415 - accuracy: 0.7961 51/126 [===========>..................] - ETA: 23s - loss: 0.5406 - accuracy: 0.7962 52/126 [===========>..................] - ETA: 22s - loss: 0.5399 - accuracy: 0.7963 53/126 [===========>..................] - ETA: 22s - loss: 0.5392 - accuracy: 0.7965 54/126 [===========>..................] - ETA: 22s - loss: 0.5385 - accuracy: 0.7965 55/126 [============>.................] - ETA: 22s - loss: 0.5379 - accuracy: 0.7966 56/126 [============>.................] - ETA: 22s - loss: 0.5372 - accuracy: 0.7967 57/126 [============>.................] - ETA: 21s - loss: 0.5365 - accuracy: 0.7968 58/126 [============>.................] - ETA: 21s - loss: 0.5357 - accuracy: 0.7970 59/126 [=============>................] - ETA: 21s - loss: 0.5349 - accuracy: 0.7972 60/126 [=============>................] - ETA: 21s - loss: 0.5342 - accuracy: 0.7973 61/126 [=============>................] - ETA: 21s - loss: 0.5335 - accuracy: 0.7975 62/126 [=============>................] - ETA: 20s - loss: 0.5328 - accuracy: 0.7976 63/126 [==============>...............] - ETA: 20s - loss: 0.5320 - accuracy: 0.7979 64/126 [==============>...............] - ETA: 20s - loss: 0.5313 - accuracy: 0.7980 65/126 [==============>...............] - ETA: 20s - loss: 0.5306 - accuracy: 0.7982 66/126 [==============>...............] - ETA: 19s - loss: 0.5299 - accuracy: 0.7985 67/126 [==============>...............] - ETA: 19s - loss: 0.5291 - accuracy: 0.7987 68/126 [===============>..............] - ETA: 19s - loss: 0.5284 - accuracy: 0.7989 69/126 [===============>..............] - ETA: 18s - loss: 0.5276 - accuracy: 0.7992 70/126 [===============>..............] - ETA: 18s - loss: 0.5268 - accuracy: 0.7995 71/126 [===============>..............] - ETA: 18s - loss: 0.5260 - accuracy: 0.7997 72/126 [================>.............] - ETA: 17s - loss: 0.5252 - accuracy: 0.8000 73/126 [================>.............] - ETA: 17s - loss: 0.5244 - accuracy: 0.8002 74/126 [================>.............] - ETA: 17s - loss: 0.5236 - accuracy: 0.8005 75/126 [================>.............] - ETA: 16s - loss: 0.5228 - accuracy: 0.8008 76/126 [=================>............] - ETA: 16s - loss: 0.5220 - accuracy: 0.8010 77/126 [=================>............] - ETA: 16s - loss: 0.5212 - accuracy: 0.8013 78/126 [=================>............] - ETA: 16s - loss: 0.5204 - accuracy: 0.8015 79/126 [=================>............] - ETA: 15s - loss: 0.5196 - accuracy: 0.8017 80/126 [==================>...........] - ETA: 15s - loss: 0.5188 - accuracy: 0.8019 81/126 [==================>...........] - ETA: 15s - loss: 0.5180 - accuracy: 0.8021 82/126 [==================>...........] - ETA: 14s - loss: 0.5172 - accuracy: 0.8023 83/126 [==================>...........] - ETA: 14s - loss: 0.5164 - accuracy: 0.8025 84/126 [===================>..........] - ETA: 14s - loss: 0.5156 - accuracy: 0.8027 85/126 [===================>..........] - ETA: 13s - loss: 0.5148 - accuracy: 0.8029 86/126 [===================>..........] - ETA: 13s - loss: 0.5140 - accuracy: 0.8031 87/126 [===================>..........] - ETA: 13s - loss: 0.5132 - accuracy: 0.8033 88/126 [===================>..........] - ETA: 12s - loss: 0.5124 - accuracy: 0.8035 89/126 [====================>.........] - ETA: 12s - loss: 0.5116 - accuracy: 0.8037 90/126 [====================>.........] - ETA: 12s - loss: 0.5107 - accuracy: 0.8039 91/126 [====================>.........] - ETA: 11s - loss: 0.5099 - accuracy: 0.8041 92/126 [====================>.........] - ETA: 11s - loss: 0.5091 - accuracy: 0.8044 93/126 [=====================>........] - ETA: 11s - loss: 0.5082 - accuracy: 0.8045 94/126 [=====================>........] - ETA: 10s - loss: 0.5074 - accuracy: 0.8047 95/126 [=====================>........] - ETA: 10s - loss: 0.5066 - accuracy: 0.8049 96/126 [=====================>........] - ETA: 10s - loss: 0.5058 - accuracy: 0.8051 97/126 [======================>.......] - ETA: 9s - loss: 0.5050 - accuracy: 0.8053  98/126 [======================>.......] - ETA: 9s - loss: 0.5042 - accuracy: 0.8055 99/126 [======================>.......] - ETA: 9s - loss: 0.5034 - accuracy: 0.8057100/126 [======================>.......] - ETA: 8s - loss: 0.5027 - accuracy: 0.8058101/126 [=======================>......] - ETA: 8s - loss: 0.5019 - accuracy: 0.8060102/126 [=======================>......] - ETA: 8s - loss: 0.5011 - accuracy: 0.8062103/126 [=======================>......] - ETA: 7s - loss: 0.5003 - accuracy: 0.8064104/126 [=======================>......] - ETA: 7s - loss: 0.4995 - accuracy: 0.8066105/126 [========================>.....] - ETA: 7s - loss: 0.4987 - accuracy: 0.8068106/126 [========================>.....] - ETA: 6s - loss: 0.4980 - accuracy: 0.8070107/126 [========================>.....] - ETA: 6s - loss: 0.4972 - accuracy: 0.8071108/126 [========================>.....] - ETA: 6s - loss: 0.4964 - accuracy: 0.8073109/126 [========================>.....] - ETA: 5s - loss: 0.4957 - accuracy: 0.8075110/126 [=========================>....] - ETA: 5s - loss: 0.4949 - accuracy: 0.8077111/126 [=========================>....] - ETA: 5s - loss: 0.4942 - accuracy: 0.8079112/126 [=========================>....] - ETA: 4s - loss: 0.4934 - accuracy: 0.8081113/126 [=========================>....] - ETA: 4s - loss: 0.4927 - accuracy: 0.8083114/126 [==========================>...] - ETA: 4s - loss: 0.4919 - accuracy: 0.8085115/126 [==========================>...] - ETA: 3s - loss: 0.4912 - accuracy: 0.8088116/126 [==========================>...] - ETA: 3s - loss: 0.4904 - accuracy: 0.8090117/126 [==========================>...] - ETA: 3s - loss: 0.4897 - accuracy: 0.8092118/126 [===========================>..] - ETA: 2s - loss: 0.4890 - accuracy: 0.8094119/126 [===========================>..] - ETA: 2s - loss: 0.4882 - accuracy: 0.8096120/126 [===========================>..] - ETA: 2s - loss: 0.4875 - accuracy: 0.8098121/126 [===========================>..] - ETA: 1s - loss: 0.4868 - accuracy: 0.8101122/126 [============================>.] - ETA: 1s - loss: 0.4861 - accuracy: 0.8103123/126 [============================>.] - ETA: 1s - loss: 0.4854 - accuracy: 0.8105124/126 [============================>.] - ETA: 0s - loss: 0.4847 - accuracy: 0.8108125/126 [============================>.] - ETA: 0s - loss: 0.4840 - accuracy: 0.8110126/126 [==============================] - ETA: 0s - loss: 0.4833 - accuracy: 0.8112126/126 [==============================] - 55s 432ms/step - loss: 0.4826 - accuracy: 0.8115 - val_loss: 0.2417 - val_accuracy: 0.9565
[INFO] saving COVID-19 detector model...
[INFO] evaluating network...
              precision    recall  f1-score   support

    negative       0.95      1.00      0.97       200
    positive       1.00      0.79      0.88        53

    accuracy                           0.96       253
   macro avg       0.97      0.90      0.93       253
weighted avg       0.96      0.96      0.95       253

[[200   0]
 [ 11  42]]
acc: 0.9565
sensitivity: 1.0000
specificity: 0.7925
[INFO] compiling model...
Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 128, 128, 3)]     0         
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 128, 128, 64)      1792      
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 128, 128, 64)      36928     
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 64, 64, 64)        0         
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 64, 64, 128)       73856     
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 64, 64, 128)       147584    
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 32, 32, 128)       0         
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 32, 32, 256)       295168    
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 32, 32, 256)       590080    
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 32, 32, 256)       590080    
_________________________________________________________________
block3_conv4 (Conv2D)        (None, 32, 32, 256)       590080    
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 16, 16, 256)       0         
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 16, 16, 512)       1180160   
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 16, 16, 512)       2359808   
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 16, 16, 512)       2359808   
_________________________________________________________________
block4_conv4 (Conv2D)        (None, 16, 16, 512)       2359808   
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 8, 8, 512)         0         
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 8, 8, 512)         2359808   
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 8, 8, 512)         2359808   
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 8, 8, 512)         2359808   
_________________________________________________________________
block5_conv4 (Conv2D)        (None, 8, 8, 512)         2359808   
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         
_________________________________________________________________
average_pooling2d_1 (Average (None, 1, 1, 512)         0         
_________________________________________________________________
flatten (Flatten)            (None, 512)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 64)                32832     
_________________________________________________________________
dropout_1 (Dropout)          (None, 64)                0         
_________________________________________________________________
dense_3 (Dense)              (None, 2)                 130       
=================================================================
Total params: 20,057,346
Trainable params: 32,962
Non-trainable params: 20,024,384
_________________________________________________________________
[INFO] training head...
  1/126 [..............................] - ETA: 1:59 - loss: 0.7841 - accuracy: 0.3750  2/126 [..............................] - ETA: 51s - loss: 0.7211 - accuracy: 0.4688   3/126 [..............................] - ETA: 52s - loss: 0.7086 - accuracy: 0.4792  4/126 [..............................] - ETA: 1:03 - loss: 0.6937 - accuracy: 0.5000  5/126 [>.............................] - ETA: 1:05 - loss: 0.6783 - accuracy: 0.5250  6/126 [>.............................] - ETA: 1:11 - loss: 0.6641 - accuracy: 0.5486  7/126 [>.............................] - ETA: 1:12 - loss: 0.6522 - accuracy: 0.5697  8/126 [>.............................] - ETA: 1:11 - loss: 0.6414 - accuracy: 0.5864  9/126 [=>............................] - ETA: 1:10 - loss: 0.6307 - accuracy: 0.6015 10/126 [=>............................] - ETA: 1:09 - loss: 0.6241 - accuracy: 0.6126 11/126 [=>............................] - ETA: 1:08 - loss: 0.6188 - accuracy: 0.6220 12/126 [=>............................] - ETA: 1:07 - loss: 0.6161 - accuracy: 0.6292 13/126 [==>...........................] - ETA: 1:06 - loss: 0.6119 - accuracy: 0.6370 14/126 [==>...........................] - ETA: 1:05 - loss: 0.6064 - accuracy: 0.6451 15/126 [==>...........................] - ETA: 1:04 - loss: 0.6008 - accuracy: 0.6526 16/126 [==>...........................] - ETA: 1:03 - loss: 0.5963 - accuracy: 0.6587 17/126 [===>..........................] - ETA: 1:01 - loss: 0.5931 - accuracy: 0.6641 18/126 [===>..........................] - ETA: 1:00 - loss: 0.5907 - accuracy: 0.6688 19/126 [===>..........................] - ETA: 59s - loss: 0.5889 - accuracy: 0.6731  20/126 [===>..........................] - ETA: 58s - loss: 0.5864 - accuracy: 0.6776 21/126 [====>.........................] - ETA: 58s - loss: 0.5844 - accuracy: 0.6816 22/126 [====>.........................] - ETA: 57s - loss: 0.5828 - accuracy: 0.6852 23/126 [====>.........................] - ETA: 56s - loss: 0.5814 - accuracy: 0.6885 24/126 [====>.........................] - ETA: 55s - loss: 0.5809 - accuracy: 0.6911 25/126 [====>.........................] - ETA: 54s - loss: 0.5799 - accuracy: 0.6938 26/126 [=====>........................] - ETA: 53s - loss: 0.5788 - accuracy: 0.6966 27/126 [=====>........................] - ETA: 52s - loss: 0.5776 - accuracy: 0.6992 28/126 [=====>........................] - ETA: 51s - loss: 0.5762 - accuracy: 0.7018 29/126 [=====>........................] - ETA: 51s - loss: 0.5753 - accuracy: 0.7041 30/126 [======>.......................] - ETA: 50s - loss: 0.5746 - accuracy: 0.7060 31/126 [======>.......................] - ETA: 49s - loss: 0.5740 - accuracy: 0.7078 32/126 [======>.......................] - ETA: 48s - loss: 0.5734 - accuracy: 0.7094 33/126 [======>.......................] - ETA: 48s - loss: 0.5729 - accuracy: 0.7107 34/126 [=======>......................] - ETA: 47s - loss: 0.5724 - accuracy: 0.7119 35/126 [=======>......................] - ETA: 46s - loss: 0.5719 - accuracy: 0.7131 36/126 [=======>......................] - ETA: 46s - loss: 0.5715 - accuracy: 0.7143 37/126 [=======>......................] - ETA: 45s - loss: 0.5712 - accuracy: 0.7154 38/126 [========>.....................] - ETA: 45s - loss: 0.5708 - accuracy: 0.7163 39/126 [========>.....................] - ETA: 44s - loss: 0.5703 - accuracy: 0.7173 40/126 [========>.....................] - ETA: 43s - loss: 0.5698 - accuracy: 0.7182 41/126 [========>.....................] - ETA: 43s - loss: 0.5693 - accuracy: 0.7190 42/126 [=========>....................] - ETA: 42s - loss: 0.5688 - accuracy: 0.7198 43/126 [=========>....................] - ETA: 42s - loss: 0.5683 - accuracy: 0.7206 44/126 [=========>....................] - ETA: 41s - loss: 0.5677 - accuracy: 0.7214 45/126 [=========>....................] - ETA: 40s - loss: 0.5672 - accuracy: 0.7221 46/126 [=========>....................] - ETA: 40s - loss: 0.5665 - accuracy: 0.7228 47/126 [==========>...................] - ETA: 39s - loss: 0.5659 - accuracy: 0.7236 48/126 [==========>...................] - ETA: 39s - loss: 0.5652 - accuracy: 0.7244 49/126 [==========>...................] - ETA: 38s - loss: 0.5646 - accuracy: 0.7251 50/126 [==========>...................] - ETA: 37s - loss: 0.5639 - accuracy: 0.7258 51/126 [===========>..................] - ETA: 37s - loss: 0.5631 - accuracy: 0.7266 52/126 [===========>..................] - ETA: 36s - loss: 0.5624 - accuracy: 0.7274 53/126 [===========>..................] - ETA: 36s - loss: 0.5616 - accuracy: 0.7281 54/126 [===========>..................] - ETA: 35s - loss: 0.5610 - accuracy: 0.7288 55/126 [============>.................] - ETA: 35s - loss: 0.5602 - accuracy: 0.7294 56/126 [============>.................] - ETA: 34s - loss: 0.5596 - accuracy: 0.7301 57/126 [============>.................] - ETA: 34s - loss: 0.5588 - accuracy: 0.7307 58/126 [============>.................] - ETA: 33s - loss: 0.5581 - accuracy: 0.7313 59/126 [=============>................] - ETA: 33s - loss: 0.5573 - accuracy: 0.7319 60/126 [=============>................] - ETA: 32s - loss: 0.5565 - accuracy: 0.7325 61/126 [=============>................] - ETA: 32s - loss: 0.5557 - accuracy: 0.7331 62/126 [=============>................] - ETA: 31s - loss: 0.5549 - accuracy: 0.7337 63/126 [==============>...............] - ETA: 31s - loss: 0.5541 - accuracy: 0.7342 64/126 [==============>...............] - ETA: 30s - loss: 0.5534 - accuracy: 0.7347 65/126 [==============>...............] - ETA: 30s - loss: 0.5526 - accuracy: 0.7353 66/126 [==============>...............] - ETA: 29s - loss: 0.5518 - accuracy: 0.7358 67/126 [==============>...............] - ETA: 29s - loss: 0.5510 - accuracy: 0.7364 68/126 [===============>..............] - ETA: 29s - loss: 0.5502 - accuracy: 0.7369 69/126 [===============>..............] - ETA: 28s - loss: 0.5493 - accuracy: 0.7374 70/126 [===============>..............] - ETA: 28s - loss: 0.5485 - accuracy: 0.7379 71/126 [===============>..............] - ETA: 27s - loss: 0.5477 - accuracy: 0.7384 72/126 [================>.............] - ETA: 27s - loss: 0.5469 - accuracy: 0.7389 73/126 [================>.............] - ETA: 26s - loss: 0.5462 - accuracy: 0.7393 74/126 [================>.............] - ETA: 26s - loss: 0.5454 - accuracy: 0.7398 75/126 [================>.............] - ETA: 25s - loss: 0.5446 - accuracy: 0.7403 76/126 [=================>............] - ETA: 25s - loss: 0.5438 - accuracy: 0.7408 77/126 [=================>............] - ETA: 24s - loss: 0.5431 - accuracy: 0.7411 78/126 [=================>............] - ETA: 23s - loss: 0.5425 - accuracy: 0.7415 79/126 [=================>............] - ETA: 23s - loss: 0.5418 - accuracy: 0.7418 80/126 [==================>...........] - ETA: 22s - loss: 0.5411 - accuracy: 0.7422 81/126 [==================>...........] - ETA: 22s - loss: 0.5404 - accuracy: 0.7426 82/126 [==================>...........] - ETA: 21s - loss: 0.5397 - accuracy: 0.7429 83/126 [==================>...........] - ETA: 21s - loss: 0.5390 - accuracy: 0.7433 84/126 [===================>..........] - ETA: 20s - loss: 0.5383 - accuracy: 0.7437 85/126 [===================>..........] - ETA: 20s - loss: 0.5377 - accuracy: 0.7441 86/126 [===================>..........] - ETA: 19s - loss: 0.5370 - accuracy: 0.7444 87/126 [===================>..........] - ETA: 19s - loss: 0.5362 - accuracy: 0.7448 88/126 [===================>..........] - ETA: 18s - loss: 0.5355 - accuracy: 0.7452 89/126 [====================>.........] - ETA: 18s - loss: 0.5348 - accuracy: 0.7457 90/126 [====================>.........] - ETA: 17s - loss: 0.5341 - accuracy: 0.7461 91/126 [====================>.........] - ETA: 17s - loss: 0.5333 - accuracy: 0.7465 92/126 [====================>.........] - ETA: 16s - loss: 0.5326 - accuracy: 0.7469 93/126 [=====================>........] - ETA: 16s - loss: 0.5319 - accuracy: 0.7474 94/126 [=====================>........] - ETA: 15s - loss: 0.5312 - accuracy: 0.7478 95/126 [=====================>........] - ETA: 15s - loss: 0.5304 - accuracy: 0.7482 96/126 [=====================>........] - ETA: 14s - loss: 0.5297 - accuracy: 0.7487 97/126 [======================>.......] - ETA: 14s - loss: 0.5290 - accuracy: 0.7491 98/126 [======================>.......] - ETA: 13s - loss: 0.5283 - accuracy: 0.7495 99/126 [======================>.......] - ETA: 13s - loss: 0.5276 - accuracy: 0.7500100/126 [======================>.......] - ETA: 12s - loss: 0.5269 - accuracy: 0.7504101/126 [=======================>......] - ETA: 12s - loss: 0.5262 - accuracy: 0.7508102/126 [=======================>......] - ETA: 11s - loss: 0.5255 - accuracy: 0.7513103/126 [=======================>......] - ETA: 11s - loss: 0.5248 - accuracy: 0.7517104/126 [=======================>......] - ETA: 10s - loss: 0.5241 - accuracy: 0.7521105/126 [========================>.....] - ETA: 10s - loss: 0.5234 - accuracy: 0.7525106/126 [========================>.....] - ETA: 9s - loss: 0.5228 - accuracy: 0.7529 107/126 [========================>.....] - ETA: 9s - loss: 0.5221 - accuracy: 0.7534108/126 [========================>.....] - ETA: 8s - loss: 0.5214 - accuracy: 0.7538109/126 [========================>.....] - ETA: 8s - loss: 0.5207 - accuracy: 0.7542110/126 [=========================>....] - ETA: 7s - loss: 0.5200 - accuracy: 0.7546111/126 [=========================>....] - ETA: 7s - loss: 0.5194 - accuracy: 0.7549112/126 [=========================>....] - ETA: 6s - loss: 0.5187 - accuracy: 0.7553113/126 [=========================>....] - ETA: 6s - loss: 0.5180 - accuracy: 0.7557114/126 [==========================>...] - ETA: 5s - loss: 0.5174 - accuracy: 0.7560115/126 [==========================>...] - ETA: 5s - loss: 0.5167 - accuracy: 0.7564116/126 [==========================>...] - ETA: 4s - loss: 0.5161 - accuracy: 0.7568117/126 [==========================>...] - ETA: 4s - loss: 0.5154 - accuracy: 0.7571118/126 [===========================>..] - ETA: 3s - loss: 0.5148 - accuracy: 0.7574119/126 [===========================>..] - ETA: 3s - loss: 0.5142 - accuracy: 0.7578120/126 [===========================>..] - ETA: 2s - loss: 0.5135 - accuracy: 0.7581121/126 [===========================>..] - ETA: 2s - loss: 0.5129 - accuracy: 0.7585122/126 [============================>.] - ETA: 1s - loss: 0.5122 - accuracy: 0.7589123/126 [============================>.] - ETA: 1s - loss: 0.5116 - accuracy: 0.7592124/126 [============================>.] - ETA: 0s - loss: 0.5109 - accuracy: 0.7596125/126 [============================>.] - ETA: 0s - loss: 0.5103 - accuracy: 0.7599126/126 [==============================] - ETA: 0s - loss: 0.5096 - accuracy: 0.7603126/126 [==============================] - 75s 592ms/step - loss: 0.5090 - accuracy: 0.7607 - val_loss: 0.2726 - val_accuracy: 0.9289
[INFO] saving COVID-19 detector model...
[INFO] evaluating network...
              precision    recall  f1-score   support

    negative       0.92      1.00      0.96       200
    positive       1.00      0.66      0.80        53

    accuracy                           0.93       253
   macro avg       0.96      0.83      0.88       253
weighted avg       0.93      0.93      0.92       253

[[200   0]
 [ 18  35]]
acc: 0.9289
sensitivity: 1.0000
specificity: 0.6604
[INFO] compiling model...
Model: "model_2"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_3 (InputLayer)            [(None, 128, 128, 3) 0                                            
__________________________________________________________________________________________________
conv1_pad (ZeroPadding2D)       (None, 134, 134, 3)  0           input_3[0][0]                    
__________________________________________________________________________________________________
conv1_conv (Conv2D)             (None, 64, 64, 64)   9472        conv1_pad[0][0]                  
__________________________________________________________________________________________________
conv1_bn (BatchNormalization)   (None, 64, 64, 64)   256         conv1_conv[0][0]                 
__________________________________________________________________________________________________
conv1_relu (Activation)         (None, 64, 64, 64)   0           conv1_bn[0][0]                   
__________________________________________________________________________________________________
pool1_pad (ZeroPadding2D)       (None, 66, 66, 64)   0           conv1_relu[0][0]                 
__________________________________________________________________________________________________
pool1_pool (MaxPooling2D)       (None, 32, 32, 64)   0           pool1_pad[0][0]                  
__________________________________________________________________________________________________
conv2_block1_1_conv (Conv2D)    (None, 32, 32, 64)   4160        pool1_pool[0][0]                 
__________________________________________________________________________________________________
conv2_block1_1_bn (BatchNormali (None, 32, 32, 64)   256         conv2_block1_1_conv[0][0]        
__________________________________________________________________________________________________
conv2_block1_1_relu (Activation (None, 32, 32, 64)   0           conv2_block1_1_bn[0][0]          
__________________________________________________________________________________________________
conv2_block1_2_conv (Conv2D)    (None, 32, 32, 64)   36928       conv2_block1_1_relu[0][0]        
__________________________________________________________________________________________________
conv2_block1_2_bn (BatchNormali (None, 32, 32, 64)   256         conv2_block1_2_conv[0][0]        
__________________________________________________________________________________________________
conv2_block1_2_relu (Activation (None, 32, 32, 64)   0           conv2_block1_2_bn[0][0]          
__________________________________________________________________________________________________
conv2_block1_0_conv (Conv2D)    (None, 32, 32, 256)  16640       pool1_pool[0][0]                 
__________________________________________________________________________________________________
conv2_block1_3_conv (Conv2D)    (None, 32, 32, 256)  16640       conv2_block1_2_relu[0][0]        
__________________________________________________________________________________________________
conv2_block1_0_bn (BatchNormali (None, 32, 32, 256)  1024        conv2_block1_0_conv[0][0]        
__________________________________________________________________________________________________
conv2_block1_3_bn (BatchNormali (None, 32, 32, 256)  1024        conv2_block1_3_conv[0][0]        
__________________________________________________________________________________________________
conv2_block1_add (Add)          (None, 32, 32, 256)  0           conv2_block1_0_bn[0][0]          
                                                                 conv2_block1_3_bn[0][0]          
__________________________________________________________________________________________________
conv2_block1_out (Activation)   (None, 32, 32, 256)  0           conv2_block1_add[0][0]           
__________________________________________________________________________________________________
conv2_block2_1_conv (Conv2D)    (None, 32, 32, 64)   16448       conv2_block1_out[0][0]           
__________________________________________________________________________________________________
conv2_block2_1_bn (BatchNormali (None, 32, 32, 64)   256         conv2_block2_1_conv[0][0]        
__________________________________________________________________________________________________
conv2_block2_1_relu (Activation (None, 32, 32, 64)   0           conv2_block2_1_bn[0][0]          
__________________________________________________________________________________________________
conv2_block2_2_conv (Conv2D)    (None, 32, 32, 64)   36928       conv2_block2_1_relu[0][0]        
__________________________________________________________________________________________________
conv2_block2_2_bn (BatchNormali (None, 32, 32, 64)   256         conv2_block2_2_conv[0][0]        
__________________________________________________________________________________________________
conv2_block2_2_relu (Activation (None, 32, 32, 64)   0           conv2_block2_2_bn[0][0]          
__________________________________________________________________________________________________
conv2_block2_3_conv (Conv2D)    (None, 32, 32, 256)  16640       conv2_block2_2_relu[0][0]        
__________________________________________________________________________________________________
conv2_block2_3_bn (BatchNormali (None, 32, 32, 256)  1024        conv2_block2_3_conv[0][0]        
__________________________________________________________________________________________________
conv2_block2_add (Add)          (None, 32, 32, 256)  0           conv2_block1_out[0][0]           
                                                                 conv2_block2_3_bn[0][0]          
__________________________________________________________________________________________________
conv2_block2_out (Activation)   (None, 32, 32, 256)  0           conv2_block2_add[0][0]           
__________________________________________________________________________________________________
conv2_block3_1_conv (Conv2D)    (None, 32, 32, 64)   16448       conv2_block2_out[0][0]           
__________________________________________________________________________________________________
conv2_block3_1_bn (BatchNormali (None, 32, 32, 64)   256         conv2_block3_1_conv[0][0]        
__________________________________________________________________________________________________
conv2_block3_1_relu (Activation (None, 32, 32, 64)   0           conv2_block3_1_bn[0][0]          
__________________________________________________________________________________________________
conv2_block3_2_conv (Conv2D)    (None, 32, 32, 64)   36928       conv2_block3_1_relu[0][0]        
__________________________________________________________________________________________________
conv2_block3_2_bn (BatchNormali (None, 32, 32, 64)   256         conv2_block3_2_conv[0][0]        
__________________________________________________________________________________________________
conv2_block3_2_relu (Activation (None, 32, 32, 64)   0           conv2_block3_2_bn[0][0]          
__________________________________________________________________________________________________
conv2_block3_3_conv (Conv2D)    (None, 32, 32, 256)  16640       conv2_block3_2_relu[0][0]        
__________________________________________________________________________________________________
conv2_block3_3_bn (BatchNormali (None, 32, 32, 256)  1024        conv2_block3_3_conv[0][0]        
__________________________________________________________________________________________________
conv2_block3_add (Add)          (None, 32, 32, 256)  0           conv2_block2_out[0][0]           
                                                                 conv2_block3_3_bn[0][0]          
__________________________________________________________________________________________________
conv2_block3_out (Activation)   (None, 32, 32, 256)  0           conv2_block3_add[0][0]           
__________________________________________________________________________________________________
conv3_block1_1_conv (Conv2D)    (None, 16, 16, 128)  32896       conv2_block3_out[0][0]           
__________________________________________________________________________________________________
conv3_block1_1_bn (BatchNormali (None, 16, 16, 128)  512         conv3_block1_1_conv[0][0]        
__________________________________________________________________________________________________
conv3_block1_1_relu (Activation (None, 16, 16, 128)  0           conv3_block1_1_bn[0][0]          
__________________________________________________________________________________________________
conv3_block1_2_conv (Conv2D)    (None, 16, 16, 128)  147584      conv3_block1_1_relu[0][0]        
__________________________________________________________________________________________________
conv3_block1_2_bn (BatchNormali (None, 16, 16, 128)  512         conv3_block1_2_conv[0][0]        
__________________________________________________________________________________________________
conv3_block1_2_relu (Activation (None, 16, 16, 128)  0           conv3_block1_2_bn[0][0]          
__________________________________________________________________________________________________
conv3_block1_0_conv (Conv2D)    (None, 16, 16, 512)  131584      conv2_block3_out[0][0]           
__________________________________________________________________________________________________
conv3_block1_3_conv (Conv2D)    (None, 16, 16, 512)  66048       conv3_block1_2_relu[0][0]        
__________________________________________________________________________________________________
conv3_block1_0_bn (BatchNormali (None, 16, 16, 512)  2048        conv3_block1_0_conv[0][0]        
__________________________________________________________________________________________________
conv3_block1_3_bn (BatchNormali (None, 16, 16, 512)  2048        conv3_block1_3_conv[0][0]        
__________________________________________________________________________________________________
conv3_block1_add (Add)          (None, 16, 16, 512)  0           conv3_block1_0_bn[0][0]          
                                                                 conv3_block1_3_bn[0][0]          
__________________________________________________________________________________________________
conv3_block1_out (Activation)   (None, 16, 16, 512)  0           conv3_block1_add[0][0]           
__________________________________________________________________________________________________
conv3_block2_1_conv (Conv2D)    (None, 16, 16, 128)  65664       conv3_block1_out[0][0]           
__________________________________________________________________________________________________
conv3_block2_1_bn (BatchNormali (None, 16, 16, 128)  512         conv3_block2_1_conv[0][0]        
__________________________________________________________________________________________________
conv3_block2_1_relu (Activation (None, 16, 16, 128)  0           conv3_block2_1_bn[0][0]          
__________________________________________________________________________________________________
conv3_block2_2_conv (Conv2D)    (None, 16, 16, 128)  147584      conv3_block2_1_relu[0][0]        
__________________________________________________________________________________________________
conv3_block2_2_bn (BatchNormali (None, 16, 16, 128)  512         conv3_block2_2_conv[0][0]        
__________________________________________________________________________________________________
conv3_block2_2_relu (Activation (None, 16, 16, 128)  0           conv3_block2_2_bn[0][0]          
__________________________________________________________________________________________________
conv3_block2_3_conv (Conv2D)    (None, 16, 16, 512)  66048       conv3_block2_2_relu[0][0]        
__________________________________________________________________________________________________
conv3_block2_3_bn (BatchNormali (None, 16, 16, 512)  2048        conv3_block2_3_conv[0][0]        
__________________________________________________________________________________________________
conv3_block2_add (Add)          (None, 16, 16, 512)  0           conv3_block1_out[0][0]           
                                                                 conv3_block2_3_bn[0][0]          
__________________________________________________________________________________________________
conv3_block2_out (Activation)   (None, 16, 16, 512)  0           conv3_block2_add[0][0]           
__________________________________________________________________________________________________
conv3_block3_1_conv (Conv2D)    (None, 16, 16, 128)  65664       conv3_block2_out[0][0]           
__________________________________________________________________________________________________
conv3_block3_1_bn (BatchNormali (None, 16, 16, 128)  512         conv3_block3_1_conv[0][0]        
__________________________________________________________________________________________________
conv3_block3_1_relu (Activation (None, 16, 16, 128)  0           conv3_block3_1_bn[0][0]          
__________________________________________________________________________________________________
conv3_block3_2_conv (Conv2D)    (None, 16, 16, 128)  147584      conv3_block3_1_relu[0][0]        
__________________________________________________________________________________________________
conv3_block3_2_bn (BatchNormali (None, 16, 16, 128)  512         conv3_block3_2_conv[0][0]        
__________________________________________________________________________________________________
conv3_block3_2_relu (Activation (None, 16, 16, 128)  0           conv3_block3_2_bn[0][0]          
__________________________________________________________________________________________________
conv3_block3_3_conv (Conv2D)    (None, 16, 16, 512)  66048       conv3_block3_2_relu[0][0]        
__________________________________________________________________________________________________
conv3_block3_3_bn (BatchNormali (None, 16, 16, 512)  2048        conv3_block3_3_conv[0][0]        
__________________________________________________________________________________________________
conv3_block3_add (Add)          (None, 16, 16, 512)  0           conv3_block2_out[0][0]           
                                                                 conv3_block3_3_bn[0][0]          
__________________________________________________________________________________________________
conv3_block3_out (Activation)   (None, 16, 16, 512)  0           conv3_block3_add[0][0]           
__________________________________________________________________________________________________
conv3_block4_1_conv (Conv2D)    (None, 16, 16, 128)  65664       conv3_block3_out[0][0]           
__________________________________________________________________________________________________
conv3_block4_1_bn (BatchNormali (None, 16, 16, 128)  512         conv3_block4_1_conv[0][0]        
__________________________________________________________________________________________________
conv3_block4_1_relu (Activation (None, 16, 16, 128)  0           conv3_block4_1_bn[0][0]          
__________________________________________________________________________________________________
conv3_block4_2_conv (Conv2D)    (None, 16, 16, 128)  147584      conv3_block4_1_relu[0][0]        
__________________________________________________________________________________________________
conv3_block4_2_bn (BatchNormali (None, 16, 16, 128)  512         conv3_block4_2_conv[0][0]        
__________________________________________________________________________________________________
conv3_block4_2_relu (Activation (None, 16, 16, 128)  0           conv3_block4_2_bn[0][0]          
__________________________________________________________________________________________________
conv3_block4_3_conv (Conv2D)    (None, 16, 16, 512)  66048       conv3_block4_2_relu[0][0]        
__________________________________________________________________________________________________
conv3_block4_3_bn (BatchNormali (None, 16, 16, 512)  2048        conv3_block4_3_conv[0][0]        
__________________________________________________________________________________________________
conv3_block4_add (Add)          (None, 16, 16, 512)  0           conv3_block3_out[0][0]           
                                                                 conv3_block4_3_bn[0][0]          
__________________________________________________________________________________________________
conv3_block4_out (Activation)   (None, 16, 16, 512)  0           conv3_block4_add[0][0]           
__________________________________________________________________________________________________
conv4_block1_1_conv (Conv2D)    (None, 8, 8, 256)    131328      conv3_block4_out[0][0]           
__________________________________________________________________________________________________
conv4_block1_1_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block1_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block1_1_relu (Activation (None, 8, 8, 256)    0           conv4_block1_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block1_2_conv (Conv2D)    (None, 8, 8, 256)    590080      conv4_block1_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block1_2_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block1_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block1_2_relu (Activation (None, 8, 8, 256)    0           conv4_block1_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block1_0_conv (Conv2D)    (None, 8, 8, 1024)   525312      conv3_block4_out[0][0]           
__________________________________________________________________________________________________
conv4_block1_3_conv (Conv2D)    (None, 8, 8, 1024)   263168      conv4_block1_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block1_0_bn (BatchNormali (None, 8, 8, 1024)   4096        conv4_block1_0_conv[0][0]        
__________________________________________________________________________________________________
conv4_block1_3_bn (BatchNormali (None, 8, 8, 1024)   4096        conv4_block1_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block1_add (Add)          (None, 8, 8, 1024)   0           conv4_block1_0_bn[0][0]          
                                                                 conv4_block1_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block1_out (Activation)   (None, 8, 8, 1024)   0           conv4_block1_add[0][0]           
__________________________________________________________________________________________________
conv4_block2_1_conv (Conv2D)    (None, 8, 8, 256)    262400      conv4_block1_out[0][0]           
__________________________________________________________________________________________________
conv4_block2_1_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block2_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block2_1_relu (Activation (None, 8, 8, 256)    0           conv4_block2_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block2_2_conv (Conv2D)    (None, 8, 8, 256)    590080      conv4_block2_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block2_2_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block2_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block2_2_relu (Activation (None, 8, 8, 256)    0           conv4_block2_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block2_3_conv (Conv2D)    (None, 8, 8, 1024)   263168      conv4_block2_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block2_3_bn (BatchNormali (None, 8, 8, 1024)   4096        conv4_block2_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block2_add (Add)          (None, 8, 8, 1024)   0           conv4_block1_out[0][0]           
                                                                 conv4_block2_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block2_out (Activation)   (None, 8, 8, 1024)   0           conv4_block2_add[0][0]           
__________________________________________________________________________________________________
conv4_block3_1_conv (Conv2D)    (None, 8, 8, 256)    262400      conv4_block2_out[0][0]           
__________________________________________________________________________________________________
conv4_block3_1_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block3_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block3_1_relu (Activation (None, 8, 8, 256)    0           conv4_block3_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block3_2_conv (Conv2D)    (None, 8, 8, 256)    590080      conv4_block3_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block3_2_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block3_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block3_2_relu (Activation (None, 8, 8, 256)    0           conv4_block3_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block3_3_conv (Conv2D)    (None, 8, 8, 1024)   263168      conv4_block3_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block3_3_bn (BatchNormali (None, 8, 8, 1024)   4096        conv4_block3_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block3_add (Add)          (None, 8, 8, 1024)   0           conv4_block2_out[0][0]           
                                                                 conv4_block3_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block3_out (Activation)   (None, 8, 8, 1024)   0           conv4_block3_add[0][0]           
__________________________________________________________________________________________________
conv4_block4_1_conv (Conv2D)    (None, 8, 8, 256)    262400      conv4_block3_out[0][0]           
__________________________________________________________________________________________________
conv4_block4_1_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block4_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block4_1_relu (Activation (None, 8, 8, 256)    0           conv4_block4_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block4_2_conv (Conv2D)    (None, 8, 8, 256)    590080      conv4_block4_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block4_2_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block4_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block4_2_relu (Activation (None, 8, 8, 256)    0           conv4_block4_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block4_3_conv (Conv2D)    (None, 8, 8, 1024)   263168      conv4_block4_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block4_3_bn (BatchNormali (None, 8, 8, 1024)   4096        conv4_block4_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block4_add (Add)          (None, 8, 8, 1024)   0           conv4_block3_out[0][0]           
                                                                 conv4_block4_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block4_out (Activation)   (None, 8, 8, 1024)   0           conv4_block4_add[0][0]           
__________________________________________________________________________________________________
conv4_block5_1_conv (Conv2D)    (None, 8, 8, 256)    262400      conv4_block4_out[0][0]           
__________________________________________________________________________________________________
conv4_block5_1_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block5_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block5_1_relu (Activation (None, 8, 8, 256)    0           conv4_block5_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block5_2_conv (Conv2D)    (None, 8, 8, 256)    590080      conv4_block5_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block5_2_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block5_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block5_2_relu (Activation (None, 8, 8, 256)    0           conv4_block5_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block5_3_conv (Conv2D)    (None, 8, 8, 1024)   263168      conv4_block5_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block5_3_bn (BatchNormali (None, 8, 8, 1024)   4096        conv4_block5_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block5_add (Add)          (None, 8, 8, 1024)   0           conv4_block4_out[0][0]           
                                                                 conv4_block5_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block5_out (Activation)   (None, 8, 8, 1024)   0           conv4_block5_add[0][0]           
__________________________________________________________________________________________________
conv4_block6_1_conv (Conv2D)    (None, 8, 8, 256)    262400      conv4_block5_out[0][0]           
__________________________________________________________________________________________________
conv4_block6_1_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block6_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block6_1_relu (Activation (None, 8, 8, 256)    0           conv4_block6_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block6_2_conv (Conv2D)    (None, 8, 8, 256)    590080      conv4_block6_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block6_2_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block6_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block6_2_relu (Activation (None, 8, 8, 256)    0           conv4_block6_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block6_3_conv (Conv2D)    (None, 8, 8, 1024)   263168      conv4_block6_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block6_3_bn (BatchNormali (None, 8, 8, 1024)   4096        conv4_block6_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block6_add (Add)          (None, 8, 8, 1024)   0           conv4_block5_out[0][0]           
                                                                 conv4_block6_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block6_out (Activation)   (None, 8, 8, 1024)   0           conv4_block6_add[0][0]           
__________________________________________________________________________________________________
conv5_block1_1_conv (Conv2D)    (None, 4, 4, 512)    524800      conv4_block6_out[0][0]           
__________________________________________________________________________________________________
conv5_block1_1_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block1_1_conv[0][0]        
__________________________________________________________________________________________________
conv5_block1_1_relu (Activation (None, 4, 4, 512)    0           conv5_block1_1_bn[0][0]          
__________________________________________________________________________________________________
conv5_block1_2_conv (Conv2D)    (None, 4, 4, 512)    2359808     conv5_block1_1_relu[0][0]        
__________________________________________________________________________________________________
conv5_block1_2_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block1_2_conv[0][0]        
__________________________________________________________________________________________________
conv5_block1_2_relu (Activation (None, 4, 4, 512)    0           conv5_block1_2_bn[0][0]          
__________________________________________________________________________________________________
conv5_block1_0_conv (Conv2D)    (None, 4, 4, 2048)   2099200     conv4_block6_out[0][0]           
__________________________________________________________________________________________________
conv5_block1_3_conv (Conv2D)    (None, 4, 4, 2048)   1050624     conv5_block1_2_relu[0][0]        
__________________________________________________________________________________________________
conv5_block1_0_bn (BatchNormali (None, 4, 4, 2048)   8192        conv5_block1_0_conv[0][0]        
__________________________________________________________________________________________________
conv5_block1_3_bn (BatchNormali (None, 4, 4, 2048)   8192        conv5_block1_3_conv[0][0]        
__________________________________________________________________________________________________
conv5_block1_add (Add)          (None, 4, 4, 2048)   0           conv5_block1_0_bn[0][0]          
                                                                 conv5_block1_3_bn[0][0]          
__________________________________________________________________________________________________
conv5_block1_out (Activation)   (None, 4, 4, 2048)   0           conv5_block1_add[0][0]           
__________________________________________________________________________________________________
conv5_block2_1_conv (Conv2D)    (None, 4, 4, 512)    1049088     conv5_block1_out[0][0]           
__________________________________________________________________________________________________
conv5_block2_1_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block2_1_conv[0][0]        
__________________________________________________________________________________________________
conv5_block2_1_relu (Activation (None, 4, 4, 512)    0           conv5_block2_1_bn[0][0]          
__________________________________________________________________________________________________
conv5_block2_2_conv (Conv2D)    (None, 4, 4, 512)    2359808     conv5_block2_1_relu[0][0]        
__________________________________________________________________________________________________
conv5_block2_2_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block2_2_conv[0][0]        
__________________________________________________________________________________________________
conv5_block2_2_relu (Activation (None, 4, 4, 512)    0           conv5_block2_2_bn[0][0]          
__________________________________________________________________________________________________
conv5_block2_3_conv (Conv2D)    (None, 4, 4, 2048)   1050624     conv5_block2_2_relu[0][0]        
__________________________________________________________________________________________________
conv5_block2_3_bn (BatchNormali (None, 4, 4, 2048)   8192        conv5_block2_3_conv[0][0]        
__________________________________________________________________________________________________
conv5_block2_add (Add)          (None, 4, 4, 2048)   0           conv5_block1_out[0][0]           
                                                                 conv5_block2_3_bn[0][0]          
__________________________________________________________________________________________________
conv5_block2_out (Activation)   (None, 4, 4, 2048)   0           conv5_block2_add[0][0]           
__________________________________________________________________________________________________
conv5_block3_1_conv (Conv2D)    (None, 4, 4, 512)    1049088     conv5_block2_out[0][0]           
__________________________________________________________________________________________________
conv5_block3_1_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block3_1_conv[0][0]        
__________________________________________________________________________________________________
conv5_block3_1_relu (Activation (None, 4, 4, 512)    0           conv5_block3_1_bn[0][0]          
__________________________________________________________________________________________________
conv5_block3_2_conv (Conv2D)    (None, 4, 4, 512)    2359808     conv5_block3_1_relu[0][0]        
__________________________________________________________________________________________________
conv5_block3_2_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block3_2_conv[0][0]        
__________________________________________________________________________________________________
conv5_block3_2_relu (Activation (None, 4, 4, 512)    0           conv5_block3_2_bn[0][0]          
__________________________________________________________________________________________________
conv5_block3_3_conv (Conv2D)    (None, 4, 4, 2048)   1050624     conv5_block3_2_relu[0][0]        
__________________________________________________________________________________________________
conv5_block3_3_bn (BatchNormali (None, 4, 4, 2048)   8192        conv5_block3_3_conv[0][0]        
__________________________________________________________________________________________________
conv5_block3_add (Add)          (None, 4, 4, 2048)   0           conv5_block2_out[0][0]           
                                                                 conv5_block3_3_bn[0][0]          
__________________________________________________________________________________________________
conv5_block3_out (Activation)   (None, 4, 4, 2048)   0           conv5_block3_add[0][0]           
__________________________________________________________________________________________________
average_pooling2d_2 (AveragePoo (None, 1, 1, 2048)   0           conv5_block3_out[0][0]           
__________________________________________________________________________________________________
flatten (Flatten)               (None, 2048)         0           average_pooling2d_2[0][0]        
__________________________________________________________________________________________________
dense_4 (Dense)                 (None, 64)           131136      flatten[0][0]                    
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 64)           0           dense_4[0][0]                    
__________________________________________________________________________________________________
dense_5 (Dense)                 (None, 2)            130         dropout_2[0][0]                  
==================================================================================================
Total params: 23,718,978
Trainable params: 131,266
Non-trainable params: 23,587,712
__________________________________________________________________________________________________
[INFO] training head...
  1/126 [..............................] - ETA: 4:31 - loss: 1.0700 - accuracy: 0.3750  2/126 [..............................] - ETA: 15s - loss: 0.9805 - accuracy: 0.4688   3/126 [..............................] - ETA: 15s - loss: 0.9487 - accuracy: 0.4931  4/126 [..............................] - ETA: 16s - loss: 0.9230 - accuracy: 0.5104  5/126 [>.............................] - ETA: 17s - loss: 0.8948 - accuracy: 0.5333  6/126 [>.............................] - ETA: 19s - loss: 0.8722 - accuracy: 0.5521  7/126 [>.............................] - ETA: 19s - loss: 0.8519 - accuracy: 0.5702  8/126 [>.............................] - ETA: 19s - loss: 0.8389 - accuracy: 0.5809  9/126 [=>............................] - ETA: 19s - loss: 0.8234 - accuracy: 0.5935 10/126 [=>............................] - ETA: 19s - loss: 0.8116 - accuracy: 0.6029 11/126 [=>............................] - ETA: 19s - loss: 0.8031 - accuracy: 0.6101 12/126 [=>............................] - ETA: 19s - loss: 0.7969 - accuracy: 0.6157 13/126 [==>...........................] - ETA: 19s - loss: 0.7916 - accuracy: 0.6208 14/126 [==>...........................] - ETA: 19s - loss: 0.7880 - accuracy: 0.6250 15/126 [==>...........................] - ETA: 19s - loss: 0.7829 - accuracy: 0.6300 16/126 [==>...........................] - ETA: 19s - loss: 0.7789 - accuracy: 0.6340 17/126 [===>..........................] - ETA: 19s - loss: 0.7742 - accuracy: 0.6383 18/126 [===>..........................] - ETA: 18s - loss: 0.7703 - accuracy: 0.6410 19/126 [===>..........................] - ETA: 18s - loss: 0.7670 - accuracy: 0.6433 20/126 [===>..........................] - ETA: 18s - loss: 0.7634 - accuracy: 0.6458 21/126 [====>.........................] - ETA: 18s - loss: 0.7595 - accuracy: 0.6485 22/126 [====>.........................] - ETA: 18s - loss: 0.7554 - accuracy: 0.6516 23/126 [====>.........................] - ETA: 17s - loss: 0.7515 - accuracy: 0.6544 24/126 [====>.........................] - ETA: 17s - loss: 0.7476 - accuracy: 0.6571 25/126 [====>.........................] - ETA: 17s - loss: 0.7438 - accuracy: 0.6598 26/126 [=====>........................] - ETA: 17s - loss: 0.7401 - accuracy: 0.6625 27/126 [=====>........................] - ETA: 17s - loss: 0.7365 - accuracy: 0.6649 28/126 [=====>........................] - ETA: 17s - loss: 0.7328 - accuracy: 0.6675 29/126 [=====>........................] - ETA: 16s - loss: 0.7291 - accuracy: 0.6702 30/126 [======>.......................] - ETA: 16s - loss: 0.7252 - accuracy: 0.6728 31/126 [======>.......................] - ETA: 16s - loss: 0.7218 - accuracy: 0.6751 32/126 [======>.......................] - ETA: 16s - loss: 0.7183 - accuracy: 0.6774 33/126 [======>.......................] - ETA: 16s - loss: 0.7147 - accuracy: 0.6797 34/126 [=======>......................] - ETA: 16s - loss: 0.7115 - accuracy: 0.6818 35/126 [=======>......................] - ETA: 16s - loss: 0.7081 - accuracy: 0.6840 36/126 [=======>......................] - ETA: 15s - loss: 0.7049 - accuracy: 0.6860 37/126 [=======>......................] - ETA: 15s - loss: 0.7016 - accuracy: 0.6880 38/126 [========>.....................] - ETA: 15s - loss: 0.6984 - accuracy: 0.6900 39/126 [========>.....................] - ETA: 15s - loss: 0.6952 - accuracy: 0.6919 40/126 [========>.....................] - ETA: 15s - loss: 0.6920 - accuracy: 0.6938 41/126 [========>.....................] - ETA: 15s - loss: 0.6891 - accuracy: 0.6956 42/126 [=========>....................] - ETA: 14s - loss: 0.6863 - accuracy: 0.6972 43/126 [=========>....................] - ETA: 14s - loss: 0.6839 - accuracy: 0.6986 44/126 [=========>....................] - ETA: 14s - loss: 0.6815 - accuracy: 0.6999 45/126 [=========>....................] - ETA: 14s - loss: 0.6791 - accuracy: 0.7013 46/126 [=========>....................] - ETA: 14s - loss: 0.6768 - accuracy: 0.7027 47/126 [==========>...................] - ETA: 13s - loss: 0.6746 - accuracy: 0.7040 48/126 [==========>...................] - ETA: 13s - loss: 0.6723 - accuracy: 0.7053 49/126 [==========>...................] - ETA: 13s - loss: 0.6700 - accuracy: 0.7066 50/126 [==========>...................] - ETA: 13s - loss: 0.6678 - accuracy: 0.7079 51/126 [===========>..................] - ETA: 13s - loss: 0.6656 - accuracy: 0.7092 52/126 [===========>..................] - ETA: 12s - loss: 0.6633 - accuracy: 0.7105 53/126 [===========>..................] - ETA: 12s - loss: 0.6613 - accuracy: 0.7117 54/126 [===========>..................] - ETA: 12s - loss: 0.6593 - accuracy: 0.7129 55/126 [============>.................] - ETA: 12s - loss: 0.6573 - accuracy: 0.7141 56/126 [============>.................] - ETA: 12s - loss: 0.6553 - accuracy: 0.7152 57/126 [============>.................] - ETA: 12s - loss: 0.6534 - accuracy: 0.7163 58/126 [============>.................] - ETA: 11s - loss: 0.6515 - accuracy: 0.7174 59/126 [=============>................] - ETA: 11s - loss: 0.6498 - accuracy: 0.7184 60/126 [=============>................] - ETA: 11s - loss: 0.6481 - accuracy: 0.7193 61/126 [=============>................] - ETA: 11s - loss: 0.6464 - accuracy: 0.7203 62/126 [=============>................] - ETA: 11s - loss: 0.6447 - accuracy: 0.7213 63/126 [==============>...............] - ETA: 11s - loss: 0.6430 - accuracy: 0.7223 64/126 [==============>...............] - ETA: 10s - loss: 0.6415 - accuracy: 0.7232 65/126 [==============>...............] - ETA: 10s - loss: 0.6400 - accuracy: 0.7240 66/126 [==============>...............] - ETA: 10s - loss: 0.6385 - accuracy: 0.7249 67/126 [==============>...............] - ETA: 10s - loss: 0.6370 - accuracy: 0.7257 68/126 [===============>..............] - ETA: 10s - loss: 0.6356 - accuracy: 0.7265 69/126 [===============>..............] - ETA: 9s - loss: 0.6341 - accuracy: 0.7274  70/126 [===============>..............] - ETA: 9s - loss: 0.6328 - accuracy: 0.7281 71/126 [===============>..............] - ETA: 9s - loss: 0.6315 - accuracy: 0.7288 72/126 [================>.............] - ETA: 9s - loss: 0.6303 - accuracy: 0.7295 73/126 [================>.............] - ETA: 9s - loss: 0.6290 - accuracy: 0.7303 74/126 [================>.............] - ETA: 9s - loss: 0.6277 - accuracy: 0.7310 75/126 [================>.............] - ETA: 8s - loss: 0.6264 - accuracy: 0.7317 76/126 [=================>............] - ETA: 8s - loss: 0.6252 - accuracy: 0.7324 77/126 [=================>............] - ETA: 8s - loss: 0.6239 - accuracy: 0.7330 78/126 [=================>............] - ETA: 8s - loss: 0.6227 - accuracy: 0.7337 79/126 [=================>............] - ETA: 8s - loss: 0.6214 - accuracy: 0.7344 80/126 [==================>...........] - ETA: 8s - loss: 0.6202 - accuracy: 0.7351 81/126 [==================>...........] - ETA: 7s - loss: 0.6190 - accuracy: 0.7357 82/126 [==================>...........] - ETA: 7s - loss: 0.6178 - accuracy: 0.7363 83/126 [==================>...........] - ETA: 7s - loss: 0.6167 - accuracy: 0.7368 84/126 [===================>..........] - ETA: 7s - loss: 0.6157 - accuracy: 0.7374 85/126 [===================>..........] - ETA: 7s - loss: 0.6146 - accuracy: 0.7379 86/126 [===================>..........] - ETA: 7s - loss: 0.6135 - accuracy: 0.7385 87/126 [===================>..........] - ETA: 6s - loss: 0.6125 - accuracy: 0.7390 88/126 [===================>..........] - ETA: 6s - loss: 0.6115 - accuracy: 0.7395 89/126 [====================>.........] - ETA: 6s - loss: 0.6105 - accuracy: 0.7400 90/126 [====================>.........] - ETA: 6s - loss: 0.6096 - accuracy: 0.7404 91/126 [====================>.........] - ETA: 6s - loss: 0.6086 - accuracy: 0.7409 92/126 [====================>.........] - ETA: 6s - loss: 0.6078 - accuracy: 0.7413 93/126 [=====================>........] - ETA: 5s - loss: 0.6069 - accuracy: 0.7417 94/126 [=====================>........] - ETA: 5s - loss: 0.6060 - accuracy: 0.7421 95/126 [=====================>........] - ETA: 5s - loss: 0.6051 - accuracy: 0.7425 96/126 [=====================>........] - ETA: 5s - loss: 0.6042 - accuracy: 0.7430 97/126 [======================>.......] - ETA: 5s - loss: 0.6032 - accuracy: 0.7434 98/126 [======================>.......] - ETA: 4s - loss: 0.6023 - accuracy: 0.7439 99/126 [======================>.......] - ETA: 4s - loss: 0.6014 - accuracy: 0.7444100/126 [======================>.......] - ETA: 4s - loss: 0.6005 - accuracy: 0.7448101/126 [=======================>......] - ETA: 4s - loss: 0.5996 - accuracy: 0.7453102/126 [=======================>......] - ETA: 4s - loss: 0.5987 - accuracy: 0.7457103/126 [=======================>......] - ETA: 4s - loss: 0.5979 - accuracy: 0.7461104/126 [=======================>......] - ETA: 3s - loss: 0.5970 - accuracy: 0.7465105/126 [========================>.....] - ETA: 3s - loss: 0.5962 - accuracy: 0.7469106/126 [========================>.....] - ETA: 3s - loss: 0.5954 - accuracy: 0.7473107/126 [========================>.....] - ETA: 3s - loss: 0.5946 - accuracy: 0.7477108/126 [========================>.....] - ETA: 3s - loss: 0.5938 - accuracy: 0.7481109/126 [========================>.....] - ETA: 3s - loss: 0.5930 - accuracy: 0.7484110/126 [=========================>....] - ETA: 2s - loss: 0.5922 - accuracy: 0.7488111/126 [=========================>....] - ETA: 2s - loss: 0.5915 - accuracy: 0.7491112/126 [=========================>....] - ETA: 2s - loss: 0.5908 - accuracy: 0.7495113/126 [=========================>....] - ETA: 2s - loss: 0.5901 - accuracy: 0.7498114/126 [==========================>...] - ETA: 2s - loss: 0.5893 - accuracy: 0.7502115/126 [==========================>...] - ETA: 1s - loss: 0.5886 - accuracy: 0.7505116/126 [==========================>...] - ETA: 1s - loss: 0.5879 - accuracy: 0.7509117/126 [==========================>...] - ETA: 1s - loss: 0.5871 - accuracy: 0.7512118/126 [===========================>..] - ETA: 1s - loss: 0.5864 - accuracy: 0.7516119/126 [===========================>..] - ETA: 1s - loss: 0.5857 - accuracy: 0.7519120/126 [===========================>..] - ETA: 1s - loss: 0.5850 - accuracy: 0.7523121/126 [===========================>..] - ETA: 0s - loss: 0.5843 - accuracy: 0.7526122/126 [============================>.] - ETA: 0s - loss: 0.5836 - accuracy: 0.7529123/126 [============================>.] - ETA: 0s - loss: 0.5829 - accuracy: 0.7533124/126 [============================>.] - ETA: 0s - loss: 0.5823 - accuracy: 0.7536125/126 [============================>.] - ETA: 0s - loss: 0.5816 - accuracy: 0.7539126/126 [==============================] - ETA: 0s - loss: 0.5810 - accuracy: 0.7542126/126 [==============================] - 30s 225ms/step - loss: 0.5804 - accuracy: 0.7545 - val_loss: 0.4320 - val_accuracy: 0.7905
[INFO] saving COVID-19 detector model...
[INFO] evaluating network...
              precision    recall  f1-score   support

    negative       0.79      1.00      0.88       200
    positive       0.00      0.00      0.00        53

    accuracy                           0.79       253
   macro avg       0.40      0.50      0.44       253
weighted avg       0.62      0.79      0.70       253

[[200   0]
 [ 53   0]]
acc: 0.7905
sensitivity: 1.0000
specificity: 0.0000
[INFO] compiling model...
Model: "model_3"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_4 (InputLayer)            [(None, 128, 128, 3) 0                                            
__________________________________________________________________________________________________
block1_conv1 (Conv2D)           (None, 63, 63, 32)   864         input_4[0][0]                    
__________________________________________________________________________________________________
block1_conv1_bn (BatchNormaliza (None, 63, 63, 32)   128         block1_conv1[0][0]               
__________________________________________________________________________________________________
block1_conv1_act (Activation)   (None, 63, 63, 32)   0           block1_conv1_bn[0][0]            
__________________________________________________________________________________________________
block1_conv2 (Conv2D)           (None, 61, 61, 64)   18432       block1_conv1_act[0][0]           
__________________________________________________________________________________________________
block1_conv2_bn (BatchNormaliza (None, 61, 61, 64)   256         block1_conv2[0][0]               
__________________________________________________________________________________________________
block1_conv2_act (Activation)   (None, 61, 61, 64)   0           block1_conv2_bn[0][0]            
__________________________________________________________________________________________________
block2_sepconv1 (SeparableConv2 (None, 61, 61, 128)  8768        block1_conv2_act[0][0]           
__________________________________________________________________________________________________
block2_sepconv1_bn (BatchNormal (None, 61, 61, 128)  512         block2_sepconv1[0][0]            
__________________________________________________________________________________________________
block2_sepconv2_act (Activation (None, 61, 61, 128)  0           block2_sepconv1_bn[0][0]         
__________________________________________________________________________________________________
block2_sepconv2 (SeparableConv2 (None, 61, 61, 128)  17536       block2_sepconv2_act[0][0]        
__________________________________________________________________________________________________
block2_sepconv2_bn (BatchNormal (None, 61, 61, 128)  512         block2_sepconv2[0][0]            
__________________________________________________________________________________________________
conv2d (Conv2D)                 (None, 31, 31, 128)  8192        block1_conv2_act[0][0]           
__________________________________________________________________________________________________
block2_pool (MaxPooling2D)      (None, 31, 31, 128)  0           block2_sepconv2_bn[0][0]         
__________________________________________________________________________________________________
batch_normalization (BatchNorma (None, 31, 31, 128)  512         conv2d[0][0]                     
__________________________________________________________________________________________________
add (Add)                       (None, 31, 31, 128)  0           block2_pool[0][0]                
                                                                 batch_normalization[0][0]        
__________________________________________________________________________________________________
block3_sepconv1_act (Activation (None, 31, 31, 128)  0           add[0][0]                        
__________________________________________________________________________________________________
block3_sepconv1 (SeparableConv2 (None, 31, 31, 256)  33920       block3_sepconv1_act[0][0]        
__________________________________________________________________________________________________
block3_sepconv1_bn (BatchNormal (None, 31, 31, 256)  1024        block3_sepconv1[0][0]            
__________________________________________________________________________________________________
block3_sepconv2_act (Activation (None, 31, 31, 256)  0           block3_sepconv1_bn[0][0]         
__________________________________________________________________________________________________
block3_sepconv2 (SeparableConv2 (None, 31, 31, 256)  67840       block3_sepconv2_act[0][0]        
__________________________________________________________________________________________________
block3_sepconv2_bn (BatchNormal (None, 31, 31, 256)  1024        block3_sepconv2[0][0]            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 16, 16, 256)  32768       add[0][0]                        
__________________________________________________________________________________________________
block3_pool (MaxPooling2D)      (None, 16, 16, 256)  0           block3_sepconv2_bn[0][0]         
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 16, 16, 256)  1024        conv2d_1[0][0]                   
__________________________________________________________________________________________________
add_1 (Add)                     (None, 16, 16, 256)  0           block3_pool[0][0]                
                                                                 batch_normalization_1[0][0]      
__________________________________________________________________________________________________
block4_sepconv1_act (Activation (None, 16, 16, 256)  0           add_1[0][0]                      
__________________________________________________________________________________________________
block4_sepconv1 (SeparableConv2 (None, 16, 16, 728)  188672      block4_sepconv1_act[0][0]        
__________________________________________________________________________________________________
block4_sepconv1_bn (BatchNormal (None, 16, 16, 728)  2912        block4_sepconv1[0][0]            
__________________________________________________________________________________________________
block4_sepconv2_act (Activation (None, 16, 16, 728)  0           block4_sepconv1_bn[0][0]         
__________________________________________________________________________________________________
block4_sepconv2 (SeparableConv2 (None, 16, 16, 728)  536536      block4_sepconv2_act[0][0]        
__________________________________________________________________________________________________
block4_sepconv2_bn (BatchNormal (None, 16, 16, 728)  2912        block4_sepconv2[0][0]            
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 8, 8, 728)    186368      add_1[0][0]                      
__________________________________________________________________________________________________
block4_pool (MaxPooling2D)      (None, 8, 8, 728)    0           block4_sepconv2_bn[0][0]         
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 8, 8, 728)    2912        conv2d_2[0][0]                   
__________________________________________________________________________________________________
add_2 (Add)                     (None, 8, 8, 728)    0           block4_pool[0][0]                
                                                                 batch_normalization_2[0][0]      
__________________________________________________________________________________________________
block5_sepconv1_act (Activation (None, 8, 8, 728)    0           add_2[0][0]                      
__________________________________________________________________________________________________
block5_sepconv1 (SeparableConv2 (None, 8, 8, 728)    536536      block5_sepconv1_act[0][0]        
__________________________________________________________________________________________________
block5_sepconv1_bn (BatchNormal (None, 8, 8, 728)    2912        block5_sepconv1[0][0]            
__________________________________________________________________________________________________
block5_sepconv2_act (Activation (None, 8, 8, 728)    0           block5_sepconv1_bn[0][0]         
__________________________________________________________________________________________________
block5_sepconv2 (SeparableConv2 (None, 8, 8, 728)    536536      block5_sepconv2_act[0][0]        
__________________________________________________________________________________________________
block5_sepconv2_bn (BatchNormal (None, 8, 8, 728)    2912        block5_sepconv2[0][0]            
__________________________________________________________________________________________________
block5_sepconv3_act (Activation (None, 8, 8, 728)    0           block5_sepconv2_bn[0][0]         
__________________________________________________________________________________________________
block5_sepconv3 (SeparableConv2 (None, 8, 8, 728)    536536      block5_sepconv3_act[0][0]        
__________________________________________________________________________________________________
block5_sepconv3_bn (BatchNormal (None, 8, 8, 728)    2912        block5_sepconv3[0][0]            
__________________________________________________________________________________________________
add_3 (Add)                     (None, 8, 8, 728)    0           block5_sepconv3_bn[0][0]         
                                                                 add_2[0][0]                      
__________________________________________________________________________________________________
block6_sepconv1_act (Activation (None, 8, 8, 728)    0           add_3[0][0]                      
__________________________________________________________________________________________________
block6_sepconv1 (SeparableConv2 (None, 8, 8, 728)    536536      block6_sepconv1_act[0][0]        
__________________________________________________________________________________________________
block6_sepconv1_bn (BatchNormal (None, 8, 8, 728)    2912        block6_sepconv1[0][0]            
__________________________________________________________________________________________________
block6_sepconv2_act (Activation (None, 8, 8, 728)    0           block6_sepconv1_bn[0][0]         
__________________________________________________________________________________________________
block6_sepconv2 (SeparableConv2 (None, 8, 8, 728)    536536      block6_sepconv2_act[0][0]        
__________________________________________________________________________________________________
block6_sepconv2_bn (BatchNormal (None, 8, 8, 728)    2912        block6_sepconv2[0][0]            
__________________________________________________________________________________________________
block6_sepconv3_act (Activation (None, 8, 8, 728)    0           block6_sepconv2_bn[0][0]         
__________________________________________________________________________________________________
block6_sepconv3 (SeparableConv2 (None, 8, 8, 728)    536536      block6_sepconv3_act[0][0]        
__________________________________________________________________________________________________
block6_sepconv3_bn (BatchNormal (None, 8, 8, 728)    2912        block6_sepconv3[0][0]            
__________________________________________________________________________________________________
add_4 (Add)                     (None, 8, 8, 728)    0           block6_sepconv3_bn[0][0]         
                                                                 add_3[0][0]                      
__________________________________________________________________________________________________
block7_sepconv1_act (Activation (None, 8, 8, 728)    0           add_4[0][0]                      
__________________________________________________________________________________________________
block7_sepconv1 (SeparableConv2 (None, 8, 8, 728)    536536      block7_sepconv1_act[0][0]        
__________________________________________________________________________________________________
block7_sepconv1_bn (BatchNormal (None, 8, 8, 728)    2912        block7_sepconv1[0][0]            
__________________________________________________________________________________________________
block7_sepconv2_act (Activation (None, 8, 8, 728)    0           block7_sepconv1_bn[0][0]         
__________________________________________________________________________________________________
block7_sepconv2 (SeparableConv2 (None, 8, 8, 728)    536536      block7_sepconv2_act[0][0]        
__________________________________________________________________________________________________
block7_sepconv2_bn (BatchNormal (None, 8, 8, 728)    2912        block7_sepconv2[0][0]            
__________________________________________________________________________________________________
block7_sepconv3_act (Activation (None, 8, 8, 728)    0           block7_sepconv2_bn[0][0]         
__________________________________________________________________________________________________
block7_sepconv3 (SeparableConv2 (None, 8, 8, 728)    536536      block7_sepconv3_act[0][0]        
__________________________________________________________________________________________________
block7_sepconv3_bn (BatchNormal (None, 8, 8, 728)    2912        block7_sepconv3[0][0]            
__________________________________________________________________________________________________
add_5 (Add)                     (None, 8, 8, 728)    0           block7_sepconv3_bn[0][0]         
                                                                 add_4[0][0]                      
__________________________________________________________________________________________________
block8_sepconv1_act (Activation (None, 8, 8, 728)    0           add_5[0][0]                      
__________________________________________________________________________________________________
block8_sepconv1 (SeparableConv2 (None, 8, 8, 728)    536536      block8_sepconv1_act[0][0]        
__________________________________________________________________________________________________
block8_sepconv1_bn (BatchNormal (None, 8, 8, 728)    2912        block8_sepconv1[0][0]            
__________________________________________________________________________________________________
block8_sepconv2_act (Activation (None, 8, 8, 728)    0           block8_sepconv1_bn[0][0]         
__________________________________________________________________________________________________
block8_sepconv2 (SeparableConv2 (None, 8, 8, 728)    536536      block8_sepconv2_act[0][0]        
__________________________________________________________________________________________________
block8_sepconv2_bn (BatchNormal (None, 8, 8, 728)    2912        block8_sepconv2[0][0]            
__________________________________________________________________________________________________
block8_sepconv3_act (Activation (None, 8, 8, 728)    0           block8_sepconv2_bn[0][0]         
__________________________________________________________________________________________________
block8_sepconv3 (SeparableConv2 (None, 8, 8, 728)    536536      block8_sepconv3_act[0][0]        
__________________________________________________________________________________________________
block8_sepconv3_bn (BatchNormal (None, 8, 8, 728)    2912        block8_sepconv3[0][0]            
__________________________________________________________________________________________________
add_6 (Add)                     (None, 8, 8, 728)    0           block8_sepconv3_bn[0][0]         
                                                                 add_5[0][0]                      
__________________________________________________________________________________________________
block9_sepconv1_act (Activation (None, 8, 8, 728)    0           add_6[0][0]                      
__________________________________________________________________________________________________
block9_sepconv1 (SeparableConv2 (None, 8, 8, 728)    536536      block9_sepconv1_act[0][0]        
__________________________________________________________________________________________________
block9_sepconv1_bn (BatchNormal (None, 8, 8, 728)    2912        block9_sepconv1[0][0]            
__________________________________________________________________________________________________
block9_sepconv2_act (Activation (None, 8, 8, 728)    0           block9_sepconv1_bn[0][0]         
__________________________________________________________________________________________________
block9_sepconv2 (SeparableConv2 (None, 8, 8, 728)    536536      block9_sepconv2_act[0][0]        
__________________________________________________________________________________________________
block9_sepconv2_bn (BatchNormal (None, 8, 8, 728)    2912        block9_sepconv2[0][0]            
__________________________________________________________________________________________________
block9_sepconv3_act (Activation (None, 8, 8, 728)    0           block9_sepconv2_bn[0][0]         
__________________________________________________________________________________________________
block9_sepconv3 (SeparableConv2 (None, 8, 8, 728)    536536      block9_sepconv3_act[0][0]        
__________________________________________________________________________________________________
block9_sepconv3_bn (BatchNormal (None, 8, 8, 728)    2912        block9_sepconv3[0][0]            
__________________________________________________________________________________________________
add_7 (Add)                     (None, 8, 8, 728)    0           block9_sepconv3_bn[0][0]         
                                                                 add_6[0][0]                      
__________________________________________________________________________________________________
block10_sepconv1_act (Activatio (None, 8, 8, 728)    0           add_7[0][0]                      
__________________________________________________________________________________________________
block10_sepconv1 (SeparableConv (None, 8, 8, 728)    536536      block10_sepconv1_act[0][0]       
__________________________________________________________________________________________________
block10_sepconv1_bn (BatchNorma (None, 8, 8, 728)    2912        block10_sepconv1[0][0]           
__________________________________________________________________________________________________
block10_sepconv2_act (Activatio (None, 8, 8, 728)    0           block10_sepconv1_bn[0][0]        
__________________________________________________________________________________________________
block10_sepconv2 (SeparableConv (None, 8, 8, 728)    536536      block10_sepconv2_act[0][0]       
__________________________________________________________________________________________________
block10_sepconv2_bn (BatchNorma (None, 8, 8, 728)    2912        block10_sepconv2[0][0]           
__________________________________________________________________________________________________
block10_sepconv3_act (Activatio (None, 8, 8, 728)    0           block10_sepconv2_bn[0][0]        
__________________________________________________________________________________________________
block10_sepconv3 (SeparableConv (None, 8, 8, 728)    536536      block10_sepconv3_act[0][0]       
__________________________________________________________________________________________________
block10_sepconv3_bn (BatchNorma (None, 8, 8, 728)    2912        block10_sepconv3[0][0]           
__________________________________________________________________________________________________
add_8 (Add)                     (None, 8, 8, 728)    0           block10_sepconv3_bn[0][0]        
                                                                 add_7[0][0]                      
__________________________________________________________________________________________________
block11_sepconv1_act (Activatio (None, 8, 8, 728)    0           add_8[0][0]                      
__________________________________________________________________________________________________
block11_sepconv1 (SeparableConv (None, 8, 8, 728)    536536      block11_sepconv1_act[0][0]       
__________________________________________________________________________________________________
block11_sepconv1_bn (BatchNorma (None, 8, 8, 728)    2912        block11_sepconv1[0][0]           
__________________________________________________________________________________________________
block11_sepconv2_act (Activatio (None, 8, 8, 728)    0           block11_sepconv1_bn[0][0]        
__________________________________________________________________________________________________
block11_sepconv2 (SeparableConv (None, 8, 8, 728)    536536      block11_sepconv2_act[0][0]       
__________________________________________________________________________________________________
block11_sepconv2_bn (BatchNorma (None, 8, 8, 728)    2912        block11_sepconv2[0][0]           
__________________________________________________________________________________________________
block11_sepconv3_act (Activatio (None, 8, 8, 728)    0           block11_sepconv2_bn[0][0]        
__________________________________________________________________________________________________
block11_sepconv3 (SeparableConv (None, 8, 8, 728)    536536      block11_sepconv3_act[0][0]       
__________________________________________________________________________________________________
block11_sepconv3_bn (BatchNorma (None, 8, 8, 728)    2912        block11_sepconv3[0][0]           
__________________________________________________________________________________________________
add_9 (Add)                     (None, 8, 8, 728)    0           block11_sepconv3_bn[0][0]        
                                                                 add_8[0][0]                      
__________________________________________________________________________________________________
block12_sepconv1_act (Activatio (None, 8, 8, 728)    0           add_9[0][0]                      
__________________________________________________________________________________________________
block12_sepconv1 (SeparableConv (None, 8, 8, 728)    536536      block12_sepconv1_act[0][0]       
__________________________________________________________________________________________________
block12_sepconv1_bn (BatchNorma (None, 8, 8, 728)    2912        block12_sepconv1[0][0]           
__________________________________________________________________________________________________
block12_sepconv2_act (Activatio (None, 8, 8, 728)    0           block12_sepconv1_bn[0][0]        
__________________________________________________________________________________________________
block12_sepconv2 (SeparableConv (None, 8, 8, 728)    536536      block12_sepconv2_act[0][0]       
__________________________________________________________________________________________________
block12_sepconv2_bn (BatchNorma (None, 8, 8, 728)    2912        block12_sepconv2[0][0]           
__________________________________________________________________________________________________
block12_sepconv3_act (Activatio (None, 8, 8, 728)    0           block12_sepconv2_bn[0][0]        
__________________________________________________________________________________________________
block12_sepconv3 (SeparableConv (None, 8, 8, 728)    536536      block12_sepconv3_act[0][0]       
__________________________________________________________________________________________________
block12_sepconv3_bn (BatchNorma (None, 8, 8, 728)    2912        block12_sepconv3[0][0]           
__________________________________________________________________________________________________
add_10 (Add)                    (None, 8, 8, 728)    0           block12_sepconv3_bn[0][0]        
                                                                 add_9[0][0]                      
__________________________________________________________________________________________________
block13_sepconv1_act (Activatio (None, 8, 8, 728)    0           add_10[0][0]                     
__________________________________________________________________________________________________
block13_sepconv1 (SeparableConv (None, 8, 8, 728)    536536      block13_sepconv1_act[0][0]       
__________________________________________________________________________________________________
block13_sepconv1_bn (BatchNorma (None, 8, 8, 728)    2912        block13_sepconv1[0][0]           
__________________________________________________________________________________________________
block13_sepconv2_act (Activatio (None, 8, 8, 728)    0           block13_sepconv1_bn[0][0]        
__________________________________________________________________________________________________
block13_sepconv2 (SeparableConv (None, 8, 8, 1024)   752024      block13_sepconv2_act[0][0]       
__________________________________________________________________________________________________
block13_sepconv2_bn (BatchNorma (None, 8, 8, 1024)   4096        block13_sepconv2[0][0]           
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 4, 4, 1024)   745472      add_10[0][0]                     
__________________________________________________________________________________________________
block13_pool (MaxPooling2D)     (None, 4, 4, 1024)   0           block13_sepconv2_bn[0][0]        
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 4, 4, 1024)   4096        conv2d_3[0][0]                   
__________________________________________________________________________________________________
add_11 (Add)                    (None, 4, 4, 1024)   0           block13_pool[0][0]               
                                                                 batch_normalization_3[0][0]      
__________________________________________________________________________________________________
block14_sepconv1 (SeparableConv (None, 4, 4, 1536)   1582080     add_11[0][0]                     
__________________________________________________________________________________________________
block14_sepconv1_bn (BatchNorma (None, 4, 4, 1536)   6144        block14_sepconv1[0][0]           
__________________________________________________________________________________________________
block14_sepconv1_act (Activatio (None, 4, 4, 1536)   0           block14_sepconv1_bn[0][0]        
__________________________________________________________________________________________________
block14_sepconv2 (SeparableConv (None, 4, 4, 2048)   3159552     block14_sepconv1_act[0][0]       
__________________________________________________________________________________________________
block14_sepconv2_bn (BatchNorma (None, 4, 4, 2048)   8192        block14_sepconv2[0][0]           
__________________________________________________________________________________________________
block14_sepconv2_act (Activatio (None, 4, 4, 2048)   0           block14_sepconv2_bn[0][0]        
__________________________________________________________________________________________________
average_pooling2d_3 (AveragePoo (None, 1, 1, 2048)   0           block14_sepconv2_act[0][0]       
__________________________________________________________________________________________________
flatten (Flatten)               (None, 2048)         0           average_pooling2d_3[0][0]        
__________________________________________________________________________________________________
dense_6 (Dense)                 (None, 64)           131136      flatten[0][0]                    
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 64)           0           dense_6[0][0]                    
__________________________________________________________________________________________________
dense_7 (Dense)                 (None, 2)            130         dropout_3[0][0]                  
==================================================================================================
Total params: 20,992,746
Trainable params: 131,266
Non-trainable params: 20,861,480
__________________________________________________________________________________________________
[INFO] training head...
  1/126 [..............................] - ETA: 3:54 - loss: 0.6062 - accuracy: 0.8750  2/126 [..............................] - ETA: 17s - loss: 0.6646 - accuracy: 0.8125   3/126 [..............................] - ETA: 17s - loss: 0.6532 - accuracy: 0.7917  4/126 [..............................] - ETA: 18s - loss: 0.6360 - accuracy: 0.7891  5/126 [>.............................] - ETA: 19s - loss: 0.6272 - accuracy: 0.7812  6/126 [>.............................] - ETA: 19s - loss: 0.6194 - accuracy: 0.7795  7/126 [>.............................] - ETA: 20s - loss: 0.6172 - accuracy: 0.7778  8/126 [>.............................] - ETA: 20s - loss: 0.6132 - accuracy: 0.7763  9/126 [=>............................] - ETA: 20s - loss: 0.6067 - accuracy: 0.7765 10/126 [=>............................] - ETA: 20s - loss: 0.6045 - accuracy: 0.7763 11/126 [=>............................] - ETA: 20s - loss: 0.5987 - accuracy: 0.7781 12/126 [=>............................] - ETA: 20s - loss: 0.5914 - accuracy: 0.7801 13/126 [==>...........................] - ETA: 20s - loss: 0.5867 - accuracy: 0.7815 14/126 [==>...........................] - ETA: 20s - loss: 0.5813 - accuracy: 0.7830 15/126 [==>...........................] - ETA: 20s - loss: 0.5756 - accuracy: 0.7842 16/126 [==>...........................] - ETA: 19s - loss: 0.5696 - accuracy: 0.7855 17/126 [===>..........................] - ETA: 19s - loss: 0.5643 - accuracy: 0.7864 18/126 [===>..........................] - ETA: 19s - loss: 0.5594 - accuracy: 0.7875 19/126 [===>..........................] - ETA: 19s - loss: 0.5542 - accuracy: 0.7889 20/126 [===>..........................] - ETA: 19s - loss: 0.5487 - accuracy: 0.7908 21/126 [====>.........................] - ETA: 19s - loss: 0.5429 - accuracy: 0.7928 22/126 [====>.........................] - ETA: 18s - loss: 0.5374 - accuracy: 0.7947 23/126 [====>.........................] - ETA: 18s - loss: 0.5322 - accuracy: 0.7965 24/126 [====>.........................] - ETA: 18s - loss: 0.5271 - accuracy: 0.7983 25/126 [====>.........................] - ETA: 18s - loss: 0.5220 - accuracy: 0.8002 26/126 [=====>........................] - ETA: 18s - loss: 0.5172 - accuracy: 0.8017 27/126 [=====>........................] - ETA: 18s - loss: 0.5129 - accuracy: 0.8034 28/126 [=====>........................] - ETA: 17s - loss: 0.5088 - accuracy: 0.8049 29/126 [=====>........................] - ETA: 17s - loss: 0.5048 - accuracy: 0.8064 30/126 [======>.......................] - ETA: 17s - loss: 0.5015 - accuracy: 0.8077 31/126 [======>.......................] - ETA: 17s - loss: 0.4985 - accuracy: 0.8087 32/126 [======>.......................] - ETA: 17s - loss: 0.4957 - accuracy: 0.8097 33/126 [======>.......................] - ETA: 16s - loss: 0.4929 - accuracy: 0.8106 34/126 [=======>......................] - ETA: 16s - loss: 0.4903 - accuracy: 0.8116 35/126 [=======>......................] - ETA: 16s - loss: 0.4875 - accuracy: 0.8125 36/126 [=======>......................] - ETA: 16s - loss: 0.4849 - accuracy: 0.8134 37/126 [=======>......................] - ETA: 16s - loss: 0.4822 - accuracy: 0.8143 38/126 [========>.....................] - ETA: 15s - loss: 0.4796 - accuracy: 0.8153 39/126 [========>.....................] - ETA: 15s - loss: 0.4769 - accuracy: 0.8162 40/126 [========>.....................] - ETA: 15s - loss: 0.4743 - accuracy: 0.8171 41/126 [========>.....................] - ETA: 15s - loss: 0.4718 - accuracy: 0.8181 42/126 [=========>....................] - ETA: 15s - loss: 0.4695 - accuracy: 0.8190 43/126 [=========>....................] - ETA: 15s - loss: 0.4672 - accuracy: 0.8199 44/126 [=========>....................] - ETA: 14s - loss: 0.4648 - accuracy: 0.8208 45/126 [=========>....................] - ETA: 14s - loss: 0.4624 - accuracy: 0.8217 46/126 [=========>....................] - ETA: 14s - loss: 0.4600 - accuracy: 0.8227 47/126 [==========>...................] - ETA: 14s - loss: 0.4576 - accuracy: 0.8236 48/126 [==========>...................] - ETA: 13s - loss: 0.4553 - accuracy: 0.8245 49/126 [==========>...................] - ETA: 13s - loss: 0.4531 - accuracy: 0.8254 50/126 [==========>...................] - ETA: 13s - loss: 0.4508 - accuracy: 0.8263 51/126 [===========>..................] - ETA: 13s - loss: 0.4486 - accuracy: 0.8271 52/126 [===========>..................] - ETA: 13s - loss: 0.4465 - accuracy: 0.8278 53/126 [===========>..................] - ETA: 13s - loss: 0.4445 - accuracy: 0.8285 54/126 [===========>..................] - ETA: 13s - loss: 0.4425 - accuracy: 0.8292 55/126 [============>.................] - ETA: 12s - loss: 0.4407 - accuracy: 0.8298 56/126 [============>.................] - ETA: 12s - loss: 0.4389 - accuracy: 0.8304 57/126 [============>.................] - ETA: 12s - loss: 0.4372 - accuracy: 0.8309 58/126 [============>.................] - ETA: 12s - loss: 0.4355 - accuracy: 0.8314 59/126 [=============>................] - ETA: 12s - loss: 0.4339 - accuracy: 0.8320 60/126 [=============>................] - ETA: 11s - loss: 0.4323 - accuracy: 0.8324 61/126 [=============>................] - ETA: 11s - loss: 0.4307 - accuracy: 0.8329 62/126 [=============>................] - ETA: 11s - loss: 0.4292 - accuracy: 0.8333 63/126 [==============>...............] - ETA: 11s - loss: 0.4277 - accuracy: 0.8337 64/126 [==============>...............] - ETA: 11s - loss: 0.4262 - accuracy: 0.8342 65/126 [==============>...............] - ETA: 11s - loss: 0.4247 - accuracy: 0.8346 66/126 [==============>...............] - ETA: 10s - loss: 0.4232 - accuracy: 0.8351 67/126 [==============>...............] - ETA: 10s - loss: 0.4217 - accuracy: 0.8355 68/126 [===============>..............] - ETA: 10s - loss: 0.4203 - accuracy: 0.8359 69/126 [===============>..............] - ETA: 10s - loss: 0.4190 - accuracy: 0.8363 70/126 [===============>..............] - ETA: 10s - loss: 0.4177 - accuracy: 0.8367 71/126 [===============>..............] - ETA: 9s - loss: 0.4165 - accuracy: 0.8371  72/126 [================>.............] - ETA: 9s - loss: 0.4153 - accuracy: 0.8374 73/126 [================>.............] - ETA: 9s - loss: 0.4140 - accuracy: 0.8378 74/126 [================>.............] - ETA: 9s - loss: 0.4128 - accuracy: 0.8382 75/126 [================>.............] - ETA: 9s - loss: 0.4116 - accuracy: 0.8386 76/126 [=================>............] - ETA: 9s - loss: 0.4104 - accuracy: 0.8389 77/126 [=================>............] - ETA: 8s - loss: 0.4092 - accuracy: 0.8393 78/126 [=================>............] - ETA: 8s - loss: 0.4081 - accuracy: 0.8396 79/126 [=================>............] - ETA: 8s - loss: 0.4069 - accuracy: 0.8400 80/126 [==================>...........] - ETA: 8s - loss: 0.4058 - accuracy: 0.8403 81/126 [==================>...........] - ETA: 8s - loss: 0.4047 - accuracy: 0.8407 82/126 [==================>...........] - ETA: 7s - loss: 0.4036 - accuracy: 0.8410 83/126 [==================>...........] - ETA: 7s - loss: 0.4026 - accuracy: 0.8413 84/126 [===================>..........] - ETA: 7s - loss: 0.4016 - accuracy: 0.8416 85/126 [===================>..........] - ETA: 7s - loss: 0.4006 - accuracy: 0.8419 86/126 [===================>..........] - ETA: 7s - loss: 0.3996 - accuracy: 0.8421 87/126 [===================>..........] - ETA: 7s - loss: 0.3986 - accuracy: 0.8424 88/126 [===================>..........] - ETA: 6s - loss: 0.3977 - accuracy: 0.8426 89/126 [====================>.........] - ETA: 6s - loss: 0.3967 - accuracy: 0.8429 90/126 [====================>.........] - ETA: 6s - loss: 0.3958 - accuracy: 0.8432 91/126 [====================>.........] - ETA: 6s - loss: 0.3948 - accuracy: 0.8435 92/126 [====================>.........] - ETA: 6s - loss: 0.3938 - accuracy: 0.8438 93/126 [=====================>........] - ETA: 5s - loss: 0.3928 - accuracy: 0.8441 94/126 [=====================>........] - ETA: 5s - loss: 0.3918 - accuracy: 0.8444 95/126 [=====================>........] - ETA: 5s - loss: 0.3909 - accuracy: 0.8447 96/126 [=====================>........] - ETA: 5s - loss: 0.3899 - accuracy: 0.8450 97/126 [======================>.......] - ETA: 5s - loss: 0.3890 - accuracy: 0.8453 98/126 [======================>.......] - ETA: 5s - loss: 0.3881 - accuracy: 0.8456 99/126 [======================>.......] - ETA: 4s - loss: 0.3872 - accuracy: 0.8459100/126 [======================>.......] - ETA: 4s - loss: 0.3863 - accuracy: 0.8462101/126 [=======================>......] - ETA: 4s - loss: 0.3855 - accuracy: 0.8465102/126 [=======================>......] - ETA: 4s - loss: 0.3846 - accuracy: 0.8468103/126 [=======================>......] - ETA: 4s - loss: 0.3838 - accuracy: 0.8471104/126 [=======================>......] - ETA: 3s - loss: 0.3829 - accuracy: 0.8474105/126 [========================>.....] - ETA: 3s - loss: 0.3821 - accuracy: 0.8477106/126 [========================>.....] - ETA: 3s - loss: 0.3813 - accuracy: 0.8479107/126 [========================>.....] - ETA: 3s - loss: 0.3805 - accuracy: 0.8482108/126 [========================>.....] - ETA: 3s - loss: 0.3797 - accuracy: 0.8485109/126 [========================>.....] - ETA: 3s - loss: 0.3790 - accuracy: 0.8487110/126 [=========================>....] - ETA: 2s - loss: 0.3782 - accuracy: 0.8490111/126 [=========================>....] - ETA: 2s - loss: 0.3775 - accuracy: 0.8493112/126 [=========================>....] - ETA: 2s - loss: 0.3767 - accuracy: 0.8495113/126 [=========================>....] - ETA: 2s - loss: 0.3760 - accuracy: 0.8498114/126 [==========================>...] - ETA: 2s - loss: 0.3752 - accuracy: 0.8500115/126 [==========================>...] - ETA: 1s - loss: 0.3745 - accuracy: 0.8503116/126 [==========================>...] - ETA: 1s - loss: 0.3738 - accuracy: 0.8505117/126 [==========================>...] - ETA: 1s - loss: 0.3731 - accuracy: 0.8508118/126 [===========================>..] - ETA: 1s - loss: 0.3723 - accuracy: 0.8510119/126 [===========================>..] - ETA: 1s - loss: 0.3716 - accuracy: 0.8513120/126 [===========================>..] - ETA: 1s - loss: 0.3709 - accuracy: 0.8515121/126 [===========================>..] - ETA: 0s - loss: 0.3702 - accuracy: 0.8518122/126 [============================>.] - ETA: 0s - loss: 0.3695 - accuracy: 0.8520123/126 [============================>.] - ETA: 0s - loss: 0.3688 - accuracy: 0.8523124/126 [============================>.] - ETA: 0s - loss: 0.3681 - accuracy: 0.8525125/126 [============================>.] - ETA: 0s - loss: 0.3674 - accuracy: 0.8527126/126 [==============================] - ETA: 0s - loss: 0.3667 - accuracy: 0.8530126/126 [==============================] - 30s 229ms/step - loss: 0.3661 - accuracy: 0.8532 - val_loss: 0.1272 - val_accuracy: 0.9565
[INFO] saving COVID-19 detector model...
[INFO] evaluating network...
              precision    recall  f1-score   support

    negative       0.97      0.97      0.97       200
    positive       0.90      0.89      0.90        53

    accuracy                           0.96       253
   macro avg       0.94      0.93      0.93       253
weighted avg       0.96      0.96      0.96       253

[[195   5]
 [  6  47]]
acc: 0.9565
sensitivity: 0.9750
specificity: 0.8868
